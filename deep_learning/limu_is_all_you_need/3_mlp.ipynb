{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d19a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from utils.loader import load_data_fashion_mnist\n",
    "from utils.train import train_ch3,predict_ch3\n",
    "from utils.animation import Animator\n",
    "from utils.accumulator import Accumulator\n",
    "\n",
    "batch_size = 256\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f'使用设备类型: {device}')\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
    "\n",
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "\n",
    "# by hand\n",
    "# 这里为什么要用nn.Parameter包装一次？因为这里的张量经过了一次计算（乘以一个常数），就变成了中间节点，不会保存梯度。需要手动再处理如下：\n",
    "# W1 = (torch.randn(num_inputs, num_hiddens, requires_grad=True,device=device) * 0.01).detach().requires_grad_(True)\n",
    "# b1 = torch.zeros(num_hiddens, requires_grad=True,device=device)\n",
    "# W2 = (torch.randn(num_hiddens, num_outputs, requires_grad=True,device=device) * 0.01).detach().requires_grad_(True)\n",
    "# b2 = torch.zeros(num_outputs, requires_grad=True,device=device)\n",
    "# 为了简单，直接使用nn.Parameter包一层，就能保证张量一定会被保留梯度\n",
    "W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=True,device=device) * 0.01)\n",
    "b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True,device=device))\n",
    "W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=True,device=device) * 0.01)\n",
    "b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True,device=device))\n",
    "\n",
    "params = [W1, b1, W2, b2]\n",
    "\n",
    "def relu(X):\n",
    "    a = torch.zeros_like(X)\n",
    "    return torch.max(X, a)\n",
    "\n",
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H = relu(X@W1 + b1)  # 这里“@”代表矩阵乘法\n",
    "    return (H@W2 + b2)\n",
    "\n",
    "def train_epoch(data_iter,loss,updater,net):\n",
    "    acc = Accumulator(3)\n",
    "    for x,y in data_iter:\n",
    "        x,y = x.to(device),y.to(device)\n",
    "        y_hat = net(x)\n",
    "        l = loss(y_hat,y)\n",
    "        updater.zero_grad()\n",
    "        l.mean().backward()\n",
    "        updater.step()\n",
    "        acc.add(l.sum(),accuracy(y,y_hat),y.numel())\n",
    "    return acc[0]/acc[2],acc[1]/acc[2]\n",
    "\n",
    "def evaluate(data_iter,net):\n",
    "    acc = Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for x,y in data_iter:\n",
    "            x,y = x.to(device),y.to(device)\n",
    "            y_hat = net(x)\n",
    "            accurate_num = accuracy(y,y_hat)\n",
    "            acc.add(accurate_num,y.numel())\n",
    "    return acc[0]/acc[1]\n",
    "\n",
    "# 计算的一个批量中准确的样本数量，不是准确率\n",
    "# 注意这里的y和y_hat是不一样的，\n",
    "# y是一个一维的向量，每个元素表示类别的索引，\n",
    "# y_hat是一个二维矩阵，0维长度是样本长度，1维长度是类别数量，数值表示概率\n",
    "def accuracy(y,y_hat:torch.Tensor):\n",
    "    y_hat = y_hat.argmax(dim=-1)\n",
    "    if y_hat.dim() > 1:\n",
    "        y_hat = y_hat.squeeze(-1)\n",
    "    y = y.to(y_hat.device)\n",
    "    return (y == y_hat).sum()\n",
    "\n",
    "def mlp_by_hand_train(net, train_iter,test_iter,loss,updater,num_epochs):\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n",
    "                        legend=['train loss', 'train acc', 'test acc'])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch(train_iter,loss,updater,net)\n",
    "        test_acc  = evaluate(test_iter,net)\n",
    "        animator.add(epoch+1,train_metrics+(test_acc,))\n",
    "\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "num_epochs, lr = 10, 0.1\n",
    "updater = torch.optim.SGD(params, lr=lr)\n",
    "mlp_by_hand_train(net,train_iter,test_iter,loss,updater,num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838e9c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train import predict_ch3\n",
    "\n",
    "predict_ch3(net, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf809226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by nn\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Flatten(),\n",
    "                    nn.Linear(784, 256,device=device),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, 10,device=device))\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "net.apply(init_weights)\n",
    "batch_size, lr, num_epochs = 128, 0.1, 20\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
    "train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
