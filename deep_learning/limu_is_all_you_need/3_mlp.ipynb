{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d19a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from utils.loader import load_data_fashion_mnist\n",
    "from utils.animation import Animator\n",
    "from utils.accumulator import Accumulator\n",
    "\n",
    "batch_size = 256\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f'使用设备类型: {device}')\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
    "\n",
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "\n",
    "# by hand\n",
    "# 这里为什么要用nn.Parameter包一次？因为这里的张量经过了一次计算（乘以一个常数），就变成了中间节点，不会保存梯度。需要手动再处理如下：\n",
    "# W1 = (torch.randn(num_inputs, num_hiddens, requires_grad=True,device=device) * 0.01).detach().requires_grad_(True)\n",
    "# b1 = torch.zeros(num_hiddens, requires_grad=True,device=device)\n",
    "# W2 = (torch.randn(num_hiddens, num_outputs, requires_grad=True,device=device) * 0.01).detach().requires_grad_(True)\n",
    "# b2 = torch.zeros(num_outputs, requires_grad=True,device=device)\n",
    "# 为了简单，直接使用nn.Parameter包一层，就能保证张量一定会被保留梯度\n",
    "W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=True,device=device) * 0.01)\n",
    "b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True,device=device))\n",
    "W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=True,device=device) * 0.01)\n",
    "b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True,device=device))\n",
    "\n",
    "params = [W1, b1, W2, b2]\n",
    "\n",
    "def relu(X):\n",
    "    a = torch.zeros_like(X)\n",
    "    return torch.max(X, a)\n",
    "\n",
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H = relu(X@W1 + b1)  # 这里“@”代表矩阵乘法\n",
    "    return (H@W2 + b2)\n",
    "\n",
    "def evaluate(data_iter,net):\n",
    "    acc = Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for x,y in data_iter:\n",
    "            x,y = x.to(device),y.to(device)\n",
    "            y_hat = net(x)\n",
    "            accurate_num = accuracy(y,y_hat)\n",
    "            acc.add(accurate_num,y.numel())\n",
    "    return acc[0]/acc[1]\n",
    "\n",
    "# 计算的一个批量中准确的样本数量，不是准确率\n",
    "# 注意这里的y和y_hat是不一样的，\n",
    "# y是一个一维的向量，每个元素表示类别的索引，\n",
    "# y_hat是一个二维矩阵，0维长度是样本长度，1维长度是类别数量，数值表示概率\n",
    "def accuracy(y,y_hat:torch.Tensor):\n",
    "    y_hat = y_hat.argmax(dim=-1)\n",
    "    if y_hat.dim() > 1:\n",
    "        y_hat = y_hat.squeeze(-1)\n",
    "    y = y.to(y_hat.device)\n",
    "    return (y == y_hat).sum().detach()\n",
    "\n",
    "def mlp_by_hand_train_epoch(data_iter,loss,updater,net):\n",
    "    acc = Accumulator(3)\n",
    "    for x,y in data_iter:\n",
    "        x,y = x.to(device),y.to(device)\n",
    "        y_hat = net(x)\n",
    "        # 注意，这里的y_hat和y的顺序不能乱\n",
    "        l = loss(y_hat,y)\n",
    "        updater.zero_grad()\n",
    "        l.mean().backward()\n",
    "        updater.step()\n",
    "        acc.add(l.sum().detach(),accuracy(y,y_hat),y.numel())\n",
    "    return acc[0]/acc[2],acc[1]/acc[2]\n",
    "\n",
    "def mlp_by_hand_train(net, train_iter,test_iter,loss,updater,num_epochs):\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.2, 0.9],\n",
    "                        legend=['train loss', 'train acc', 'test acc'])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = mlp_by_hand_train_epoch(train_iter,loss,updater,net)\n",
    "        test_acc  = evaluate(test_iter,net)\n",
    "        animator.add(epoch+1,train_metrics+(test_acc,))\n",
    "\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "num_epochs, lr = 10, 0.1\n",
    "updater = torch.optim.SGD(params, lr=lr)\n",
    "mlp_by_hand_train(net,train_iter,test_iter,loss,updater,num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838e9c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train import show_images\n",
    "\n",
    "def get_label_text_by_idx(label_idxs):\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[idx] for idx in label_idxs]\n",
    "\n",
    "def mlp_by_hand_predict(net, data_iter, n=6):\n",
    "    first_batch = next(iter(data_iter))\n",
    "    x,y = first_batch\n",
    "    x = x.to(device)\n",
    "    y_hat = net(x)\n",
    "    y_hat_idxs = y_hat.argmax(-1)\n",
    "    preds = get_label_text_by_idx(y_hat_idxs)\n",
    "    trues = get_label_text_by_idx(y)\n",
    "    titles = [true + '\\n' + pred for true,pred in zip(trues, preds)]\n",
    "    show_images(x[:n].to('cpu').reshape(-1,28,28),1,n,titles=titles)\n",
    "\n",
    "mlp_by_hand_predict(net,test_iter,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf809226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by nn\n",
    "from torch import nn\n",
    "import torch\n",
    "from utils.loader import load_data_fashion_mnist\n",
    "from utils.animation import Animator\n",
    "from utils.accumulator import Accumulator\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f'使用设备类型: {device}')\n",
    "net = nn.Sequential(nn.Flatten(),\n",
    "                    nn.Linear(784, 256,device=device),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, 10,device=device))\n",
    "\n",
    "def init_model(layer):\n",
    "    if type(layer) == nn.Linear:\n",
    "        nn.init.normal_(layer.weight,0,0.01)\n",
    "        nn.init.zeros_(layer.bias)\n",
    "\n",
    "net.apply(init_model)\n",
    "\n",
    "batch_size, lr, num_epochs = 256, 0.1, 10\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
    "\n",
    "# y_hat是二维矩阵,y 是一维向量\n",
    "def accuracy(y_hat:torch.Tensor,y:torch.Tensor):\n",
    "    with torch.no_grad():\n",
    "        y_hat = y_hat.argmax(-1)\n",
    "        return (y == y_hat).sum().detach()\n",
    "\n",
    "def evaluate(data_iter,net):\n",
    "    acc = Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for x,y in data_iter:\n",
    "            x,y = x.to(device),y.to(device)\n",
    "            y_hat = net(x)\n",
    "            accurate_num = accuracy(y_hat,y)\n",
    "            acc.add(accurate_num,y.numel())\n",
    "    return acc[0]/acc[1]\n",
    "\n",
    "def mlp_by_nn_train_epoch(net: nn.Module,train_iter, loss, trainer:torch.optim.Optimizer):\n",
    "    acc = Accumulator(3)\n",
    "    for x, y in train_iter:\n",
    "        x,y = x.to(device),y.to(device)\n",
    "        y_hat = net(x)\n",
    "        l = loss(y_hat,y)\n",
    "        trainer.zero_grad()\n",
    "        l.mean().backward()\n",
    "        trainer.step()\n",
    "        acc.add(float(l.sum().detach()),accuracy(y_hat,y),y.numel())\n",
    "    return acc[0]/acc[2],acc[1]/acc[2]\n",
    "\n",
    "\n",
    "def mlp_by_nn_train(net:nn.Module,train_iter,test_iter,loss,trainer,num_epoch):\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.2, 0.9],\n",
    "                legend=['train loss', 'train acc', 'test acc'])\n",
    "    for epoch in range(num_epoch):\n",
    "        # 切换到训练模式\n",
    "        net.train()\n",
    "        metrics = mlp_by_nn_train_epoch(net,train_iter,loss,trainer)\n",
    "        # 切换到评估模式\n",
    "        net.eval()\n",
    "        test_acc = evaluate(test_iter,net)\n",
    "        metrics = metrics + (test_acc,)\n",
    "        animator.add(epoch+1,metrics)\n",
    "            \n",
    "mlp_by_nn_train(net, train_iter, test_iter, loss, trainer,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a14fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "def get_label_text_by_idx(labels)->list[str]:\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "            'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[label_idx] for label_idx in labels]\n",
    "\n",
    "# 这里打印出第一个测试集第一个batch预测错误的\n",
    "def mlp_by_nn_predict(data_iter,net,n=6):\n",
    "    x,y = next(iter(data_iter))\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    y_hat = net(x)\n",
    "    y_hat_idx = y_hat.argmax(dim=-1)\n",
    "    nonequal_idx = torch.where(y_hat_idx != y)[0]\n",
    "    if nonequal_idx.numel() == 0:\n",
    "        print('当前batch没有预测错误的样本')\n",
    "        return\n",
    "    idx_show = nonequal_idx[:n]\n",
    "    nonequal_preds_idx = y_hat_idx[idx_show]\n",
    "    nonequal_trues_idx = y[idx_show]\n",
    "\n",
    "    nonequal_preds = get_label_text_by_idx(nonequal_preds_idx.detach().cpu().tolist())\n",
    "    nonequal_trues = get_label_text_by_idx(nonequal_trues_idx.detach().cpu().tolist())\n",
    "    titles = [true + '\\n' + pred for true, pred in zip(nonequal_trues, nonequal_preds)]\n",
    "    show_images(x[idx_show].detach().cpu().reshape(-1,28,28), 1, len(idx_show), titles)\n",
    "\n",
    "mlp_by_nn_predict(test_iter,net,4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
