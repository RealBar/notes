{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73df6ea",
   "metadata": {},
   "source": [
    "# 线性神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927e464",
   "metadata": {},
   "source": [
    "### 线性回归by hand\n",
    "#### 理论\n",
    "数学模型:\n",
    "$$\n",
    "\\hat y = \\mathbf w^T\\mathbf x\n",
    "$$\n",
    "回归问题一般选MSE做损失函数：\n",
    "$$\n",
    "L(\\mathbf w,b) = \\frac 1 n\\sum_{i=1}^n \\frac 1 2 \\big(\\mathbf w^T\\mathbf x +b-y^{(i)}\\big)^2\n",
    "$$\n",
    "目标：\n",
    "$$\n",
    "\\mathbf w^{best},b^{best} = \\operatorname*\\argmin _{\\mathbf w,b} L(\\mathbf w,b)\n",
    "$$\n",
    "\n",
    "\n",
    "另：一条重要结论  \n",
    "在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。证明过程：  \n",
    "于假设显性模型的输出中增加了高斯噪声\n",
    "$$y=\\mathbf w^T\\mathbf x + b + \\epsilon, 其中 \\epsilon \\sim \\mathcal N(0,\\sigma^2) $$\n",
    "因此可以写出给定x观测到y的似然：\n",
    "$$P(y|\\mathbf x) = \\frac 1 {\\sqrt {2\\pi}\\sigma}exp\\Big(-\\frac {1} {2\\sigma^2} (y - \\mathbf w^T\\mathbf x - b) \\Big)$$\n",
    "然后根据极大似然估计: **模型参数的最优值是使整个数据集的似然最大的值**。整个数据集都发生的似然为（样本集满足i.i.d假设情况下）：\n",
    "$$\n",
    "\\mathbf w^{best},b^{best}=\\argmax_{\\mathbf w,b} p(\\mathbf y | \\mathbf X) = \\argmax_{\\mathbf w,b}\\prod_{i=1}^{n}p(y^{(i)}|\\mathbf x^{(i)})\n",
    " $$\n",
    "对目标函数p(y|X)取负对数得到损失函数：\n",
    "$$\n",
    "-\\log  p(\\mathbf y | \\mathbf X) = \\sum_{i=1}^n \\frac 1 2 \\log (2\\pi \\sigma^2) + \\frac 1 {2\\sigma^2}\\Big( y^{(i)} - \\mathbf w^T\\mathbf x^{(i)} - b\\Big)^2\n",
    "$$\n",
    "最后一个式子即等价于均方误差MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcaa2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.timer import Timer\n",
    "from utils.accumulator import Accumulator\n",
    "from utils.animation import Animator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420f7809",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f'使用设备类型: {device}')\n",
    "\n",
    "\n",
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"生成y=Xw+b+噪声\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)),device=device)\n",
    "    y = X @ w + b\n",
    "    y += torch.normal(0, 0.01, y.shape,device=device)\n",
    "    return X, y[..., None] ## 为啥要升维？\n",
    "\n",
    "true_w = torch.tensor([2, -3.4],dtype=torch.float32,device=device)\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n",
    "print(f'前5个特征: {features[:5]}, \\n前5个标签:{labels[:5]}\\n形状:feature{list(features.shape)}, label{labels.shape}')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# 散点图的x和y的shape要一致，因此要features, labels)会报错\n",
    "x = features[:, 0].squeeze() # 这里不 squeeze 也可以，因为 scatter 会自动处理 1 维张量\n",
    "y = labels.squeeze()\n",
    "plt.scatter(x.cpu(), y.cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e63036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 训练相关准备\n",
    "# 随机迭代\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num = len(features)\n",
    "    indices = list(range(num))\n",
    "    random.shuffle(indices)\n",
    "    for i in range (0,num,batch_size):\n",
    "        batch_indices = indices[i:min(i+batch_size,num)]\n",
    "        yield features[batch_indices],labels[batch_indices]\n",
    "# 初始化模型参数\n",
    "batch_size = 10\n",
    "for X,y in data_iter(batch_size,features,labels):\n",
    "    print(X,'\\n',y)\n",
    "    break\n",
    "w = torch.normal(0,0.01,(2,1),requires_grad=True,device=device)\n",
    "b = torch.zeros(1,requires_grad=True,device=device)\n",
    "\n",
    "# 模型输出函数\n",
    "def line_regression(X:torch.Tensor,w:torch.Tensor,b:torch.Tensor):\n",
    "    return X.matmul(w) + b\n",
    "# 损失函数\n",
    "def mse_loss(y:torch.Tensor,y_hat:torch.Tensor):\n",
    "    return ((y.squeeze() - y_hat.squeeze())**2)/2\n",
    "# 优化方法，注意这里为什么要除以batch_size?\n",
    "def sgd(params:list[torch.Tensor], lr, batch_size):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad/batch_size\n",
    "            param.grad.zero_() ## 注意这里要清零\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c0c3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "lr = 0.03\n",
    "epoch_num = 5\n",
    "net = line_regression\n",
    "loss = mse_loss\n",
    "\n",
    "for epoch_idx in range(epoch_num):\n",
    "    for X,y in data_iter(batch_size,features,labels):\n",
    "        y_hat = net(X,w,b)\n",
    "        l = loss(y,y_hat)\n",
    "        l.sum().backward()\n",
    "        sgd([w,b],lr,batch_size)\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(labels,net(features,w,b))\n",
    "        print(f'epoch{epoch_idx+1}, mean loss {float(train_l.mean()):.5f}')\n",
    "print(f'finshed train, ground truth w:{true_w.tolist()},b:{true_b}\\nour model param w:{w.tolist()},b:{b.tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc14fec",
   "metadata": {},
   "source": [
    "### 总结一下\n",
    "先说一个误区：MSE的计算公式是包含求平均这个操作的\n",
    "$$\n",
    "MSE = \\frac 1 n \\sum_{i=1}^n (y^{(i)} - \\hat y^{(i)})\n",
    "$$\n",
    "但是李沐的代码里，mse计算的时候并没有计算这个1/n的平均，但是在sgd里减梯度的时候除以了一个batch_size\n",
    "\n",
    "模型训练三要素：\n",
    "- net：模型本身，其实是一个将输入的批量样本映射成一个输出的映射函数，它的参数包含**批量输入的特征X、模型参数（包含w，b）**\n",
    "- loss：损失函数，只负责将模型输出的估计值和真实的样本标签（两个都是批量的）映射为一个损失值（也是批量），它的输入只有**估计值和真实标签**\n",
    "- 优化函数：主要负责对模型本身进行参数更新\n",
    "\n",
    "每个batch的处理流程：\n",
    "1. 批量特征数据输入模型，得到批量估计输出\n",
    "2. 根据估计输出和标签计算loss（loss也是批量的）\n",
    "3. loss.sum().backword()，此时梯度会存储在模型的各个参数\n",
    "4. 使用优化器更新模型参数，优化器内部会读取模型的梯度，自动更新。注意优化器在更新模型参数时要with torch.no_grad()，更新完成后要net.grad.zero_()清除本次的梯度，否则会累积"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea40bbc6",
   "metadata": {},
   "source": [
    "### 线性回归by nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db9b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from torch import nn\n",
    "\n",
    "true_w = torch.tensor([7,-3.45,3.767,667.32,5.2,-1136.22,0.32],dtype=torch.float32,device=device)\n",
    "true_b = torch.tensor(12.76,dtype=torch.float32,device=device)\n",
    "n = 10000\n",
    "features,labels = synthetic_data(true_w,true_b,n)\n",
    "print(f'前5个特征: {features[:5]}\\n前5个标签: {labels[:5]}\\n形状 feature:{features.shape}, labels:{labels.shape}')\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = data.DataLoader(data.TensorDataset(features,labels),batch_size,True)\n",
    "net = nn.Sequential(nn.Linear(7,1,device=device))\n",
    "net[0].weight.data.normal_(0,0.01)\n",
    "net[0].bias.data.fill_(0)\n",
    "loss = nn.MSELoss()\n",
    "trainer = torch.optim.SGD(net.parameters(),lr=0.03)\n",
    "\n",
    "# 训练\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        l = loss(net(X) ,y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    l = loss(net(features), labels)\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')\n",
    "\n",
    "print(f'原始w:\\t\\t{true_w.tolist()}, b:{true_b.tolist()}')\n",
    "print(f'训练结束后w:\\t{net[0].weight.data.squeeze().tolist()}, b:{net[0].bias.data.squeeze().tolist()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff10e98",
   "metadata": {},
   "source": [
    "总结一下：\n",
    "\n",
    "1. 使用torch.utils包下的data.TensorDataset和data.DataLoader实现数据的乱序批量读取\n",
    "2. 使用torch.optim.SGD优化器时，需要传入全部的模型参数，对于nn下的模型，直接使用xx.parameters()即可返回。注意parameters()方法会返回所有模型的参数\n",
    "3. 使用nn.Sequential和nn.Linear组合实现模型层，nn下的模型都默认requires_grad=True\n",
    "4. 使用nn.MSELoss()计算损失，注意这个返回的是平均的损失了，是一个标量\n",
    "5. backward之前，一定要记得trainer.zero_grad()\n",
    "6. backward\n",
    "7. trainer.step()\n",
    "\n",
    "> 实现的时候踩了一个坑，在生成数据的时候，true_w和true_b都使用了requires_grad=True，导致生成的features, labels都带梯度了，训练的时候报错\n",
    "\n",
    "另，做一个实验看一下nn.Sequential和nn.Linear的参数构成，可以看到，Sequential的参数即全部参数的累加，线性层的参数有一个矩阵和偏执，矩阵的参数的0,1维分别为输出和输入\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c0d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Sequential(\n",
    "    nn.Linear(10,256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256,1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024,10)\n",
    "    )\n",
    "b = list(a.parameters())\n",
    "len(b)\n",
    "for p in b: \n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f486e66",
   "metadata": {},
   "source": [
    "### softmax by hand\n",
    "#### 理论\n",
    "从网络结构上，softmax本质上是多输出的线性层，然后在输出层做一次softmax归一化。softmax回归用于做分类。分类问题一般选择交叉熵做损失函数\n",
    "softmax函数的表达式为：\n",
    "$$\n",
    "\\hat {y_j} =softmax(o_j) = \\frac {\\exp(o_j)} {\\sum_{k=1}^q \\exp(o_k)}\n",
    "$$\n",
    "其中 $o_j$ 表示模型的输出的第j个元素， $\\hat y_j$ 表示模型的预测结果为第j个类别的概率。依然是走最大似然的老路，在i.i.d假设下，整个数据集发生的概率取负对数似然为\n",
    "$$\n",
    "-\\log P(\\mathbf Y|\\mathbf X) = \\sum_{i=0}^n-\\log P(\\mathbf y^{(i)}|\\mathbf x^{(i)})=\\sum_{i=0}^n l(\\hat{\\mathbf y^{(i)}},\\mathbf y^{(i)})\n",
    "$$\n",
    "l为损失函数，这里取交叉熵损失函数\n",
    "$$\n",
    "l(\\mathbf y,\\hat{\\mathbf y}) = -\\sum_{j=1}^q y_j\\log \\hat y_j\n",
    "$$\n",
    "这里向量y是一个长度为q的one-hot编码向量，q为类别数，j表示第j个类别。带入 $\\hat {y_j}$ 的定义\n",
    "$$\n",
    "l( \\mathbf y,\\hat{\\mathbf y}) = -\\sum_{j=1}^q y_j\\log  \\frac {\\exp(o_j)} {\\sum_{k=1}^q \\exp(o_k)}\\\\\n",
    "= \\sum_{j=1}^q y_j\\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j\n",
    "$$\n",
    "上式第一项的最外层求和是对j，而内层的求和对象是y_j乘以一个跟j无关的表达式，所以可以把这个式子看成常数提出来，得到\n",
    "$$\n",
    "l(\\mathbf y,\\hat{\\mathbf y}) = \\log \\sum_{k=1}^q \\exp(o_k)\\sum_{j=1}^q y_j - \\sum_{j=1}^q y_j o_j\\\\\n",
    "= \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j\n",
    "$$\n",
    "对某个类别的输出 $o_j$ 做偏导\n",
    "$$\n",
    "\\frac {\\partial l(\\mathbf y,\\hat{\\mathbf y})} {\\partial o_j} = \\frac {\\exp(o_j)} {\\sum_{k=1}^q \\exp(o_k)} - y_j \\\\\n",
    "= softmax(o_j) - y_j\n",
    "$$\n",
    "可以看到，梯度是模型输出概率到真实概率one-hot编码的差值，是不是和mse的梯度表达式很像呢？\n",
    "\n",
    "尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个线性模型（linear model）。\n",
    "### 信息论初步\n",
    "信息量是指一个事件发生的不确定性的度量，或者说它是表示某个事件发生后我们的惊异程度。\n",
    "\n",
    "设X为一个随机变量，它的取值空间为 $ \\mathcal X$，事件 $x \\in \\mathcal X$，其发生的概率为 $P(x)$ ，其信息量\n",
    "$$\n",
    "I(x) = - \\log P(x)\n",
    "$$\n",
    "注意，信息量的计算对象是具体的事件，而熵的计算对象是随机变量的分布。熵可以看做对信息量求真实分布下的期望\n",
    "$$\n",
    "H(P) = - \\sum_{x \\in \\mathcal X} P(x) \\log P(x)\n",
    "$$\n",
    "交叉熵是使用左分布编码右分布的编码量，它的表达式为\n",
    "$$\n",
    "H(P,Q) = - \\sum_{x \\in \\mathcal X} P(x) \\log Q(x)\n",
    "$$\n",
    "更形象的表述：我们可以把交叉熵想象为“主观概率为Q的观察者在看到根据概率P生成的数据时的预期惊异”，两个分布越接近，交叉熵越小，当P=Q时，交叉熵取最小值就等于H(P)。\n",
    "\n",
    "而KL散度则是纯粹衡量两个概率之间的差异，可以理解为“距离”。当P=Q时，KL散度为0，说明两个分布是完全相同的。\n",
    "$$\n",
    "D_{KL}(P||Q) =\\sum_{x \\in \\mathcal X} P(x) \\log \\frac{P(x)}{Q(x)}  \\\\\n",
    "= H(P,Q) - H(P)\n",
    "$$\n",
    "\n",
    "### softmax by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828616db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "trans = transforms.ToTensor()\n",
    "mnist_train = torchvision.datasets.FashionMNIST(root='../../data', train=True, download=True, transform=trans)\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root='../../data', train=False, download=True, transform=trans)\n",
    "\n",
    "#结构：list of tuple (Tensor(1,28,28), label_idx)\n",
    "\n",
    "# 将数字标签转换为文本标签\n",
    "def get_fashion_mnist_labels(labels):\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[label] for label in labels]\n",
    "\n",
    "# 画图\n",
    "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save\n",
    "    \"\"\"绘制图像列表\"\"\"\n",
    "    figsize = (num_cols * scale, num_rows * scale)\n",
    "    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
    "        if torch.is_tensor(img):\n",
    "            # 图片张量\n",
    "            ax.imshow(img.numpy())\n",
    "        else:\n",
    "            # PIL图片\n",
    "            ax.imshow(img)\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "    return axes\n",
    "\n",
    "X,y = next(iter(data.DataLoader(mnist_train, batch_size=18, shuffle=True)))\n",
    "show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c58bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练准备\n",
    "batch_size = 256\n",
    "data_loader_workers = 4\n",
    "def load_data_fashion_mnist(batch_size,resize=None):\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0,transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root='../../data',train=True,download=True,transform=trans)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root='../../data',train=False,download=True,transform=trans)\n",
    "    return (data.DataLoader(mnist_train,batch_size,shuffle=True,num_workers=data_loader_workers),\n",
    "            data.DataLoader(mnist_test,batch_size,shuffle=False,num_workers=data_loader_workers))\n",
    "\n",
    "train_iter,test_iter = load_data_fashion_mnist(batch_size)\n",
    "num_input = 784 # 28*28\n",
    "num_output = 10 # 10 classes\n",
    "W = torch.normal(0,0.01,size=(num_input,num_output),device=device,dtype=torch.float32,requires_grad=True)\n",
    "b = torch.zeros(num_output,device=device,dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "def softmax(x):\n",
    "    x_exp = torch.exp(x)\n",
    "    partition = x_exp.sum(1,keepdim=True)\n",
    "    return x_exp / partition\n",
    "\n",
    "def net(X):\n",
    "    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)\n",
    "\n",
    "# 损失函数，注意这里的两个输入形状和值的意义是不同的\n",
    "# y_hat是(batch_size,num_output)表示每个类别的概率，y是(batch_size,)表示类别在one-hot编码中的索引\n",
    "# 学到了：在Tensor的索引中使用Iterable，表示按顺序取数据，比如a[[0,1],1]->[a[0,1],a[1,1]]，a[[0,1],[0,1]]->[a[0,0],a[1,1]]\n",
    "# 但是使用slice会比较有趣, a[slice(2),1]->[a[0,1],a[1,1]], 但是a[slice(2),slice(2)] 结果会变成一个矩阵\n",
    "def cross_entropy(y_hat,y):\n",
    "    return -torch.log(y_hat[range(len(y_hat)),y])\n",
    "\n",
    "def accuracy(y_hat,y):\n",
    "    y_hat = y_hat.argmax(axis=-1)\n",
    "    if y.ndim > 1:\n",
    "        y = y.squeeze(-1)\n",
    "    y = y.to(y_hat.device)\n",
    "    return (y_hat == y).sum()\n",
    "\n",
    "\n",
    "def evaluate_accuracy(net, data_iter):  #@save\n",
    "    \"\"\"计算在指定数据集上模型的精度\"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()  # 将模型设置为评估模式\n",
    "    metric = Accumulator(2)  # 正确预测数、预测总数\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "def train_epoch_ch3(net, train_iter, loss, updater):  #@save\n",
    "    \"\"\"训练模型一个迭代周期（定义见第3章）\"\"\"\n",
    "    # 将模型设置为训练模式\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.train()\n",
    "    # 训练损失总和、训练准确度总和、样本数\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # 计算梯度并更新参数\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            # 使用PyTorch内置的优化器和损失函数\n",
    "            updater.zero_grad()\n",
    "            l.mean().backward()\n",
    "            updater.step()\n",
    "        else:\n",
    "            # 使用定制的优化器和损失函数\n",
    "            l.sum().backward()\n",
    "            updater(X.shape[0])\n",
    "        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n",
    "    # 返回训练损失和训练精度\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]\n",
    "\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save\n",
    "    \"\"\"训练模型（定义见第3章）\"\"\"\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n",
    "                        legend=['train loss', 'train acc', 'test acc'])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n",
    "        test_acc = evaluate_accuracy(net, test_iter)\n",
    "        animator.add(epoch + 1, train_metrics + (test_acc,))\n",
    "    train_loss, train_acc = train_metrics\n",
    "    assert train_loss < 0.5, train_loss\n",
    "    assert train_acc <= 1 and train_acc > 0.7, train_acc\n",
    "    assert test_acc <= 1 and test_acc > 0.7, test_acc\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "def updater(batch_size):\n",
    "    return sgd([W, b], lr, batch_size)\n",
    "\n",
    "num_epochs = 10\n",
    "train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d87ca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ch3(net, test_iter, n=6):  #@save\n",
    "    \"\"\"预测标签（定义见第3章）\"\"\"\n",
    "    for X, y in test_iter:\n",
    "        break\n",
    "    X_dev = X.to(device)\n",
    "    y_hat = net(X_dev)\n",
    "    pred_idx = y_hat.argmax(dim=1).detach().cpu()\n",
    "    trues = get_fashion_mnist_labels(y[:n].tolist())\n",
    "    preds = get_fashion_mnist_labels(pred_idx[:n].tolist())\n",
    "    titles = [true +'\\n' + pred for true, pred in zip(trues, preds)]\n",
    "    show_images(X[:n].reshape((n, 28, 28)), 1, n, titles=titles)\n",
    "\n",
    "predict_ch3(net, test_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
