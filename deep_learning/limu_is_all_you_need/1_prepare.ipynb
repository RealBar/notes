{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b09fae9",
   "metadata": {},
   "source": [
    "# 准备\n",
    "### 内存优化\n",
    "如果直接复用现有变量存储Y = X + Y，会浪费一次内存分配。 我们也可以使用X[:] = X + Y或X += Y来减少操作的内存开销。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "764c28b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "X = torch.Tensor([1,2,3])\n",
    "Y = torch.Tensor([4,5,6])\n",
    "before = id(X)\n",
    "X += Y\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e8110d",
   "metadata": {},
   "source": [
    "### 张量的常规操作\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d0b6f1",
   "metadata": {},
   "source": [
    "下面按深度学习中最常用的模式，整理 PyTorch 张量（Tensor）的高频操作：初始化、算术运算、取值/索引、维度变换、聚合统计、类型与设备、自动求导，以及计算性能优化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4c8b238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch  # 导入 PyTorch：提供 Tensor、算子、自动求导等核心能力\n",
    "# ----  # 分隔：用注释行替代空行，便于逐行解释\n",
    "torch.manual_seed(42)  # 固定随机种子：让 rand/randn 等随机初始化可复现（不同设备/版本仍可能有细微差异）\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")  # 选择计算设备：优先 Apple MPS，其次 CUDA GPU，否则 CPU；对比硬编码 'cuda' 更鲁棒\n",
    "device  # 在 notebook 中显示当前 device，便于确认张量将被放在哪个设备上\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931000fd",
   "metadata": {},
   "source": [
    "#### 0）张量的基本属性\n",
    "- 张量可以表示高纬度数据，例如向量、矩阵、张量等，但也可以表示标量。\n",
    "- 升维可以使用`unsqueeze`方法，例如`a.unsqueeze(0)`可以在第0维升维，这个方法等价于a = a[None, :]，但后者需要知道当前a的维度，或者使用`...`代表前边所有维度，例如`a[..., None]`或者`a[..., None,:]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17c9c5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2]),\n",
       " torch.Size([2]),\n",
       " 1,\n",
       " tensor(1),\n",
       " torch.Size([]),\n",
       " 0,\n",
       " tensor([[1, 2]]),\n",
       " torch.Size([1, 2]),\n",
       " 2,\n",
       " tensor([[1],\n",
       "         [2]]),\n",
       " torch.Size([2, 1]),\n",
       " 2,\n",
       " tensor([1]),\n",
       " tensor([[[ 0.9047,  0.2227,  0.1460,  0.7360]],\n",
       " \n",
       "         [[-0.4142, -0.1106,  0.4540, -3.3259]],\n",
       " \n",
       "         [[-0.2307, -0.9187,  0.2464,  1.0246]]], device='mps:0'),\n",
       " torch.Size([3, 1, 4]),\n",
       " 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2]) ## 1维张量\n",
    "b = torch.tensor(1) ## 0维张量\n",
    "c = a[None, :]  ## 2维张量 等价于c = a.unsqueeze(0)，第0维添加一个维度1，shape:[2]->[1,2]\n",
    "d = a[:, None] ## 2维张量，列向量，等价于d = a.unsqueeze(1)，第1维添加一个维度1 shape:[2]->[2,1]\n",
    "e = b[None] ## 将标量转化为向量，等价于e = b.unsqueeze(0)，第0维添加一个维度1 shape:[]->[1]\n",
    "f = torch.randn(3,4,device=device)\n",
    "g = f[..., None,:]\n",
    "a,a.shape,a.dim(),b,b.shape,b.dim(),c,c.shape,c.dim(),d,d.shape,d.dim(),e,g,g.shape,g.dim()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f1d0d5",
   "metadata": {},
   "source": [
    "#### 1) 初始化（创建张量）\n",
    "- 从 Python / NumPy 构造：`torch.tensor`\n",
    "- 常用形状初始化：`zeros/ones/full/empty/eye`\n",
    "- 序列：`arange/linspace/logspace`，linspace和arange的区别是：linspace的起始值和结束值是固定的，中间的元素是根据步数`step=(end-start)/(steps-1)`算到的；arange是固定起始值，在起始值上增加步长step（linspace 的第三个参数叫steps，arange的第三个参数叫step）\n",
    "- 随机：`rand/randn/randint/normal`\n",
    "- `*_like`：保持形状/类型/设备一致：`zeros_like/ones_like/rand_like`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a0b71d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2.],\n",
       "         [3., 4.]], device='mps:0'),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]], device='mps:0'),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.]], device='mps:0'),\n",
       " tensor([[3.1400, 3.1400],\n",
       "         [3.1400, 3.1400]], device='mps:0'),\n",
       " tensor([[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]], device='mps:0'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32, device=device)  # 从 Python 列表创建张量：会拷贝数据；对比 torch.as_tensor/torch.from_numpy（CPU 上）可共享底层内存\n",
    "b = torch.zeros((2, 3), device=device)  # 全 0 初始化：常用于 mask/累加缓存；对比 torch.empty 更快但值未初始化（必须立刻写入）\n",
    "c = torch.ones_like(b)  # 形状/类型/设备跟随 b：比 torch.ones(b.shape, device=b.device, dtype=b.dtype) 更不易写错\n",
    "d = torch.full((2, 2), 3.14, device=device)  # 常数填充：对比 ones/zeros 更通用，适合初始化 bias 或构造常量张量\n",
    "e = torch.eye(3, device=device)  # 单位矩阵：常用于线性代数/初始化；对比 torch.diag(torch.ones(n)) 更直接且语义清晰\n",
    "# ----  # 分隔：序列类初始化\n",
    "f = torch.arange(0, 10, step=2, device=device)  # 等差序列：更像 Python range；对比 linspace 用 steps 指定点数而不是步长\n",
    "g = torch.linspace(0, 1, steps=5, device=device)  # 区间等分采样：适合连续区间；对比 arange 在浮点步长下可能出现累计误差/终点缺失\n",
    "# ----  # 分隔：随机初始化\n",
    "h = torch.rand((2, 2), device=device)  # U(0,1) 均匀分布：常用于随机噪声/采样；对比 randn 是 N(0,1) 正态\n",
    "i = torch.randn((2, 2), device=device)  # N(0,1) 正态分布：常用于权重初始化；对比 normal(mean,std) 可自定义分布参数\n",
    "j = torch.randint(low=0, high=10, size=(2, 3), device=device)  # 离散均匀整数：常用于类别/索引；对比 rand 需要缩放+取整且分布不精确\n",
    "# ----  # 分隔：展示结果\n",
    "a, b, c, d, e  # 返回多个对象让 notebook 一次性展示；对比 print 会把张量转成字符串，后续不便继续链式操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c8c343",
   "metadata": {},
   "source": [
    "#### 2) 算术运算与广播（Broadcasting）\n",
    "- 逐元素：`+ - * / **`，`torch.add/sub/mul/div`\n",
    "- 矩阵乘：`@` / `matmul` / `mm` / `bmm`\n",
    "- 常用逐元素函数：`exp/log/sqrt/abs/clamp`\n",
    "- 广播：对齐尾维度，满足相等或为 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6e2b1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  1.,  11., 102.],\n",
       "         [  4.,  14., 105.]], device='mps:0'),\n",
       " tensor([[ 0.1000,  2.0000, 12.0000],\n",
       "         [ 3.1000,  5.0000, 15.0000]], device='mps:0'),\n",
       " tensor([[ 0.,  1.,  4.],\n",
       "         [ 9., 16., 25.]], device='mps:0'),\n",
       " tensor([[1., 1., 2.],\n",
       "         [3., 4., 4.]], device='mps:0'),\n",
       " tensor(0., device='mps:0'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(6, device=device, dtype=torch.float32).reshape(2, 3)  # 先生成 0..5，再 reshape 成 2x3；对比 view 只在 contiguous 时可用，reshape 更通用（必要时会拷贝）\n",
    "y = torch.tensor([1.0, 10.0, 100.0], device=device)  # 1x3 向量：与 x(2x3) 运算会触发广播（broadcasting）\n",
    "# ----  # 分隔：逐元素算术（elementwise）\n",
    "elem = x + y  # 逐元素加法 + 广播：语法最简洁；对比 torch.add 更适合需要 out/alpha 或更函数式的写法\n",
    "scaled = torch.add(x, y, alpha=0.1)  # 计算 x + 0.1*y：一个算子完成缩放+相加；对比先 y*0.1 再加会多一次中间张量分配\n",
    "powed = x ** 2  # 幂运算：等价 torch.pow(x, 2)；对比 x*x 在指数为 2 时通常更快且更省内存\n",
    "clamped = torch.clamp(x, min=1.0, max=4.0)  # 截断到区间：[1,4]；对比 relu 只截断下界、hardtanh 类似双边截断\n",
    "absed = torch.abs(x - y)  # 逐元素取绝对值：等价 torch.abs(x - y)；对比 x-y 直接相减可能得到负数（如 x=2, y=3）\n",
    "exped = torch.exp(x)  # 逐元素指数：等价 torch.exp(x)；对比 math.exp 只作用于标量\n",
    "# ----  # 分隔：矩阵乘（matmul）\n",
    "m1 = torch.randn(2, 3, device=device)  # 随机矩阵：shape(2,3)\n",
    "m2 = torch.randn(3, 4, device=device)  # 随机矩阵：shape(3,4)\n",
    "mat = m1 @ m2  # 矩阵乘：等价 torch.matmul(m1, m2)；对比 torch.mm 只支持 2D，matmul 还支持 batch 维度 ## 对比 torch.mm 只支持 2D，matmul 还支持 batch 维度\n",
    "# mat_alt = m1 * m2  # 逐元素乘：等价 torch.mul(m1, m2)；对比 @ 不支持广播，需手动处理\n",
    "# ----  # 分隔：爱因斯坦求和（einsum）\n",
    "eins = torch.einsum('ij,jk->ik', m1, m2)  # 爱因斯坦求和：表达更灵活（可写复杂张量收缩）；对比 @ 更直观，且底层通常更容易走高性能实现\n",
    "# ----  # 分隔：数值一致性检查\n",
    "elem, scaled, powed, clamped, (mat - eins).abs().max()  # 比较 matmul 与 einsum 的最大误差：应接近 0（浮点误差除外）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a5f3d",
   "metadata": {},
   "source": [
    "#### 3) 取值、索引与切片\n",
    "- 基本切片：`x[i]`、`x[:, j]`、`x[..., -1]`\n",
    "- 布尔掩码：`x[mask]`\n",
    "- 选取：`index_select`、`gather`（常用于 embedding / attention）\n",
    "- 原地赋值：切片赋值会直接修改原张量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f6b5a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3], device='mps:0'),\n",
       " tensor([ 2,  6, 10], device='mps:0'),\n",
       " tensor([[ 5,  6],\n",
       "         [ 9, 10]], device='mps:0'),\n",
       " tensor([ 0,  2,  4,  6,  8, 10], device='mps:0'),\n",
       " tensor([[ 8,  9, 10, 11],\n",
       "         [ 0,  1,  2,  3]], device='mps:0'),\n",
       " tensor([[1., 2., 3.],\n",
       "         [4., 5., 6.]], device='mps:0'),\n",
       " tensor([[1., 2., 3.],\n",
       "         [5., 4., 6.]], device='mps:0'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12, device=device).reshape(3, 4)  # 构造 3x4 矩阵：用连续整数便于直观看出索引位置；对比 randn 不易辨认\n",
    "x0 = x[0]  # 取第 0 行（shape: 4）；对比 x[0:1] 会保留维度（shape: 1x4），对后续广播更友好\n",
    "xcol = x[:, 2]  # 取第 2 列（shape: 3）；对比 x[:, 2:3] 会保留列维度（shape: 3x1）\n",
    "xsub = x[1:, 1:3]  # 切片：行 1..end，列 1..2；切片通常返回视图（view），对比高级索引常返回拷贝\n",
    "# ----  # 分隔：布尔掩码（mask）\n",
    "mask = x % 2 == 0  # 生成 bool 掩码：True 表示选中；对比 torch.where 更适合做条件替换/返回坐标\n",
    "xeven = x[mask]  # 按掩码取值会展平为 1D；对比 masked_select 行为类似但更显式：torch.masked_select(x, mask)\n",
    "# ----  # 分隔：按索引选取（index_select）\n",
    "idx = torch.tensor([2, 0], device=device)  # 行索引：要取第 2 行和第 0 行；对比 Python list 索引在 GPU 上不可用\n",
    "rows = torch.index_select(x, dim=0, index=idx)  # 沿 dim=0 选行；对比 x[idx] 更简洁，但 index_select 在代码审阅时更明确\n",
    "# ----  # 分隔：gather（逐元素按坐标取值）\n",
    "src = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], device=device)  # 源矩阵：2x3\n",
    "picked = torch.gather(src, dim=0, index=torch.tensor([[0, 0, 0], [1, 1, 1]], device=device))  # gather的作用是根据index从src中取出对应元素，组成与index相同shape的tensor\n",
    "scatter_idx = torch.tensor([[0, 1], [1, 0]], device=device)  # 每个输入位置给出要放的列索引（dim=1）；对比 scatter_nd 只能整体放列\n",
    "scattered = torch.scatter(src, dim=1, index=scatter_idx, src=picked)  # scatter的作用是根据index将src中的元素放到对应位置，组成与src相同shape的tensor\n",
    "# ----  # 分隔：展示结果\n",
    "x0, xcol, xsub, xeven, rows, picked, scattered  # 汇总展示：对比不同索引方式的输出形状与内容\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1051951",
   "metadata": {},
   "source": [
    "解释gather和scatter的dim和index的关系，假设我们有一个 3D 张量，dim 的不同取值决定了哪个坐标轴被替换：  \n",
    "如果 dim=0：out[i][j][k] = input[ index[i][j][k] ][j][k]  \n",
    "如果 dim=1：out[i][j][k] = input[i][ index[i][j][k] ][k]  \n",
    "如果 dim=2：out[i][j][k] = input[i][j][ index[i][j][k] ]  \n",
    "也就是说，index的值决定了dim轴上的索引，其他轴上的索引保持不变。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fdfc7a",
   "metadata": {},
   "source": [
    "#### 4) 维度变换与拼接\n",
    "- 形状：`reshape/view/flatten`\n",
    "- 维度重排：`transpose/permute/movedim`\n",
    "- 增减维：`unsqueeze/squeeze`\n",
    "- 拼接与堆叠：`cat/stack`；拆分：`chunk/split`\n",
    "- 广播扩展：`expand`（不复制数据）、`repeat`（复制数据）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0f6c4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 4]),\n",
       " torch.Size([2, 3, 2, 2]),\n",
       " torch.Size([2, 12]),\n",
       " torch.Size([2, 4, 3]),\n",
       " torch.Size([4, 2, 3]),\n",
       " torch.Size([1, 2, 3, 4]),\n",
       " tensor(True, device='mps:0'),\n",
       " torch.Size([4, 3]),\n",
       " torch.Size([2, 2, 3]),\n",
       " torch.Size([2, 3]),\n",
       " torch.Size([4, 3]),\n",
       " torch.Size([4, 3]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(24, device=device).reshape(2, 3, 4)  # 构造 3D 张量：常见形状 (batch, seq, hidden) 或 (N, C, L)\n",
    "r1 = x.reshape(6, 4)  # 改变形状：必要时会拷贝；对比 view 仅在 contiguous 时可用但通常不拷贝（更轻量）\n",
    "r2 = x.reshape(2, 3,-1,2)  # 如果某维为 -1，则自动计算该维大小以保持元素总数不变\n",
    "flat = x.flatten(start_dim=1)  # 从第 1 维开始打平：常用于 (N,C,H,W)->(N,C*H*W)；对比 reshape 更直观、少算维度\n",
    "# ----  # 分隔：维度重排\n",
    "t = x.transpose(1, 2)  # 交换两个维度：只支持两维互换；对比 permute 可任意重排多个维度\n",
    "p = x.permute(2, 0, 1)  # 任意维度重排：更通用；对比 transpose 更简洁但能力更弱\n",
    "# ----  # 分隔：增减维\n",
    "u = x.unsqueeze(0)  # 增加长度为 1 的维度：常用于对齐 batch 维或做广播；对比 reshape 插维更容易写错\n",
    "s = u.squeeze(0)  # 去掉指定的长度为 1 的维度：与 unsqueeze 成对；对比 squeeze() 不指定 dim 可能误删其他 1 维\n",
    "# ----  # 分隔：拼接/堆叠/拆分\n",
    "a = torch.zeros((2, 3), device=device)  # 准备张量 a：用于对比 cat 与 stack\n",
    "b = torch.ones((2, 3), device=device)  # 准备张量 b：与 a 同形状\n",
    "cat0 = torch.cat([a, b], dim=0)  # 沿已有维度拼接：dim=0 变长 (4,3)；对比 stack 会新增一维\n",
    "stk0 = torch.stack([a, b], dim=0)  # 新增一维再堆叠：结果 (2,2,3)；对比 cat 更适合把 batch 维拼大\n",
    "c1, c2 = cat0.chunk(2, dim=0)  # 均匀拆分：把 (4,3) 拆成两个 (2,3)；对比 split 可按指定长度不等分\n",
    "# ----  # 分隔：广播扩展\n",
    "base = torch.arange(3, device=device).reshape(1, 3)  # 基础行向量：shape(1,3)\n",
    "expanded = base.expand(4, 3)  # 视图式扩展：不复制数据（stride 可能为 0）；对比 repeat 会真实复制、占更多内存\n",
    "repeated = base.repeat(4, 1)  # 数据复制：得到独立内存；对比 expand 适合只读广播，repeat 适合后续需要原地写/独立修改\n",
    "# ----  # 分隔：展示各操作的形状与一致性\n",
    "r1.shape,r2.shape, flat.shape, t.shape, p.shape, u.shape, (s == x).all(), cat0.shape, stk0.shape, (c1 + c2).shape, expanded.shape, repeated.shape  # 用 shape 对比各方法对维度的影响\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b22d5f",
   "metadata": {},
   "source": [
    "#### 5) 聚合统计与常用数学\n",
    "- 规约：`sum/mean/max/min`，配合 `dim` 与 `keepdim`\n",
    "- 位置：`argmax/argmin/topk`\n",
    "- 累积：`cumsum/cumprod`\n",
    "- 稳定计算：`logsumexp`、`softmax`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c53fa01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " torch.Size([2, 1, 1]),\n",
       " torch.Size([2, 3]),\n",
       " torch.Size([2, 3]),\n",
       " tensor([[0.7034, 0.3875],\n",
       "         [1.7145, 0.5452]], device='mps:0'),\n",
       " tensor([[3, 0],\n",
       "         [3, 1]], device='mps:0'),\n",
       " tensor([1.7490, 2.1497], device='mps:0'),\n",
       " tensor([1.0000, 1.0000], device='mps:0'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4, device=device)  # 构造 3D 随机张量：用于演示 dim 维度规约\n",
    "s1 = x.sum(dim=2)  # 沿 dim=2 求和：把最后一维规约掉；对比 sum() 不给 dim 会把所有元素求和得到标量\n",
    "m1 = x.mean(dim=(1, 2), keepdim=True)  # 多维求均值并保留维度：keepdim 便于后续广播；对比不 keepdim 会少维度导致广播需手动 unsqueeze\n",
    "mx = x.max(dim=-1).values  # max 返回 (values, indices)：这里只取最大值；对比 torch.amax 只返回值但不返回位置\n",
    "am = x.argmax(dim=-1)  # 最大值位置：常用于分类预测；对比 argmax 不给 dim 会在展平后找全局最大位置\n",
    "# ----  # 分隔：top-k\n",
    "scores = torch.randn(2, 5, device=device)  # 2 个样本，每个 5 个分数（logits）\n",
    "topv, topi = torch.topk(scores, k=2, dim=-1)  # 取每行最大的 2 个值及其索引；对比 sort 更通用但更慢且会做全量排序\n",
    "# ----  # 分隔：数值稳定的归一化\n",
    "ls = torch.logsumexp(scores, dim=-1)  # log(sum(exp(x))) 的稳定写法：对比 log(exp(x).sum()) 容易上溢/下溢\n",
    "sm = torch.softmax(scores, dim=-1)  # softmax：把 logits 变成概率分布；对比 sigmoid 用于二分类/多标签，不会对全向量做归一\n",
    "# ----  # 分隔：快速自检\n",
    "s1.shape, m1.shape, mx.shape, am.shape, topv, topi, ls, sm.sum(dim=-1)  # softmax 每行和应为 1（允许浮点误差）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63a0e3d",
   "metadata": {},
   "source": [
    "#### 6) 类型、设备与自动求导\n",
    "- 类型：`to(dtype=...)`、`float/long/half`\n",
    "- 设备：`to(device)`、`cpu/cuda`\n",
    "- 梯度：`requires_grad_()`、`backward()`、`detach()`\n",
    "- 常见：loss 标量对参数求导，`grad` 存在 `param.grad`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b0f4b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.553070306777954, torch.Size([3, 1]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.randn(3, 1, device=device, requires_grad=True)  # 参数张量：requires_grad=True 才会在 backward 时累积梯度；对比 w.requires_grad_(True) 是原地设置\n",
    "x = torch.randn(8, 3, device=device)  # 输入特征：8 个样本、3 维特征；对比把 x 设 requires_grad 多用于需要输入梯度的任务（如对抗样本）\n",
    "y = torch.randn(8, 1, device=device)  # 目标值：这里用回归形式演示（实际训练多来自数据集）\n",
    "# ----  # 分隔：前向计算\n",
    "pred = x @ w  # 线性层前向：等价 torch.matmul(x, w)；对比 torch.nn.Linear 会管理参数与 bias，更适合构建模型\n",
    "loss = ((pred - y) ** 2).mean()  # MSE 损失：先平方再平均；对比 torch.nn.functional.mse_loss 更标准且支持不同 reduction\n",
    "loss.backward()  # 反向传播：从 loss 触发计算图求导，并把梯度累加到 w.grad；对比 torch.autograd.grad 返回梯度但不累积\n",
    "# ----  # 分隔：取标量与查看梯度\n",
    "loss.item(), w.grad.shape  # item() 转 Python 标量（可能触发设备同步）；对比 loss.detach() 保持张量形态便于批量日志\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2c1f2d",
   "metadata": {},
   "source": [
    "#### 7) 计算性能与内存优化（常用做法）\n",
    "- 尽量用向量化/矩阵化替代 Python 循环\n",
    "- 训练外推理：`torch.no_grad()` 或 `torch.inference_mode()`\n",
    "- 原地操作减少分配：`x+=y` 等价与 `x[:]=x+y`都会避免一次额外的内存分配，但是如果直接`x=x+y`会导致一次额外的内存分配\n",
    "- 避免频繁 `cpu()` / `cuda()` 往返，减少小张量 `item()`\n",
    "- 对 GPU：`pin_memory` + `non_blocking=True` 传输；混合精度 `autocast`；`torch.compile`（可用时）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a39c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, torch.Size([1024, 1024]), True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1024, 1024, device=device)  # 构造大矩阵：便于观察算子是否产生额外内存分配/缓存\n",
    "y = torch.randn(1024, 1024, device=device)  # 同形状矩阵：用于逐元素加法与矩阵乘\n",
    "# ----  # 分隔：原地 vs 非原地（内存/性能）\n",
    "z1 = x + y  # 非原地加法：会新分配一个张量保存结果；对比 x.add_(y) 会直接改写 x，减少一次分配\n",
    "x2 = x.clone()  # 克隆一份，保留梯度追踪，重新分配内存\n",
    "x2.add_(y)  # 原地加法：把结果写回 x2；对比 x2 += y 语义类似，但 add_ 更显式也更常见于性能敏感代码\n",
    "x3 = x[:, :]  # 全切片：不复制数据，共享内存，保留原有的梯度追踪\n",
    "x4 = x.detach()  # 分离张量：不复制数据，共享内存，不跟踪梯度\n",
    "# ----  # 分隔：关闭梯度追踪（推理/评估）\n",
    "with torch.no_grad():  # 禁用 autograd：节省显存/内存并加速推理；对比 inference_mode 进一步减少开销但限制更多\n",
    "    z2 = (x @ y).relu()  # 在 no_grad 中做 matmul+ReLU：不会构建计算图；对比训练时通常不包 no_grad\n",
    "# ----  # 分隔：torch.compile（可用时）\n",
    "compiled = None  # 先设占位：在不支持 compile 的版本上也能正常运行\n",
    "if hasattr(torch, 'compile'):  # 兼容性判断：PyTorch 2.x 才有 torch.compile\n",
    "    f = lambda a, b: (a @ b).relu()  # 定义待编译函数：示例用 matmul+relu；对比写成 def 更易调试，性能目标一致\n",
    "    compiled = torch.compile(f)  # 编译加速（可能有首次编译开销）；对比 torch.jit.script/trace 是另一套图机制\n",
    "# ----  # 分隔：一致性与返回值\n",
    "(z1 - x2).abs().max().item(), z2.shape, compiled is not None  # 检查原地/非原地结果一致，返回 z2 形状与 compile 可用性\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c27336",
   "metadata": {},
   "source": [
    "### 自动梯度\n",
    "如果不设置 `requires_grad=True`，则默认不跟踪梯度。注意有两个成员变量：\n",
    "- `requires_grad`：是否跟踪梯度\n",
    "- `grad`：梯度值\n",
    "grad只有发生了反向传播，才会被计算出来，否则为None，而且注意**只有在叶子节点才会被保留**。\n",
    "\n",
    "我们看个例子: \n",
    "$\\\\ b=2a^Ta \\\\ c = b^3$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1ad72733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tm/k6np_6fj0hs9sybswfd_xrjw0000gn/T/ipykernel_91533/1211071561.py:5: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  c,c.grad,b,b.grad,a,a.grad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(85184000., device='mps:0', grad_fn=<PowBackward0>),\n",
       " None,\n",
       " tensor(440., device='mps:0', grad_fn=<MulBackward0>),\n",
       " None,\n",
       " tensor([ 0.,  2.,  4.,  6.,  8., 10.], device='mps:0', requires_grad=True),\n",
       " tensor([       0.,  4646400.,  9292800., 13939200., 18585600., 23232000.],\n",
       "        device='mps:0'))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.linspace(0,10,6,dtype=torch.float32,device=device,requires_grad=True)\n",
    "b = a.dot(a) * 2\n",
    "c = b**3\n",
    "c.backward()\n",
    "c,c.grad,b,b.grad,a,a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40781465",
   "metadata": {},
   "source": [
    "可以看到，c的梯度和b的梯度都是None，只有叶子节点的a有梯度  \n",
    "\n",
    "另外，我们调用反向计算时，一般都是在标量上调用，因为如果是在向量上调用，会得到一个矩阵，所以如果一定要在向量上调用，我们一般先sum一下，把向量变成标量，或者backword函数传入一个ones_like这个向量的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "732d027d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 2., 4., 6., 8.], device='mps:0', requires_grad=True),\n",
       " tensor([ 0.,  4.,  8., 12., 16.], device='mps:0'),\n",
       " tensor([1., 1., 1., 1., 1.], device='mps:0'))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(0,10,2,requires_grad=True,dtype=torch.float32,device=device)\n",
    "b = a*a\n",
    "# b.backward() 这样会报错\"grad can be implicitly created only for scalar outputs\"，因为b不是一个标量\n",
    "# b.sum().backward()\n",
    "b.backward(torch.ones_like(b))\n",
    "a,a.grad,torch.ones_like(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6e7824",
   "metadata": {},
   "source": [
    "### 分离计算\n",
    "当某个变量之前的计算过程不用考虑，用detach分离出来，detach方法返回的可以看做一个常量，它的梯度永远是 None。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6234a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True], device='mps:0')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad.zero_()\n",
    "b = a*a\n",
    "u = b.detach()\n",
    "c = u*a\n",
    "c.sum().backward()\n",
    "a.grad == u \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea3610",
   "metadata": {},
   "source": [
    "### 自动微分的传染性\n",
    "对于一个变量a required_grad=True，它和其他变量、常量计算的结果，只要没有with torch.no_grad()，结果就一定是required_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcbf236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(1,15,3,dtype=torch.float32,requires_grad=True,device=device) ## 自动微分\n",
    "b = torch.full(a.shape,2,dtype=torch.float32,requires_grad=False,device=device) ## 不自动微分\n",
    "c = a*b\n",
    "with torch.no_grad():\n",
    "    d = a*b\n",
    "# d.sum().backward() 会报错\n",
    "c.requires_grad,d.requires_grad # True, False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
