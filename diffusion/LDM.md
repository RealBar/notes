LDM学习笔记
> https://arxiv.org/pdf/2112.10752

# 为什么VAE效果糊，但是LDM=VAE+diffusion效果好
这里贴一个很精彩的知乎回答：
SD里用的那个VAE，跟你教科书里学的那个只用MSE Loss和KL Loss训练出来的原始VAE，压根就不是一个物种。如果把这个前提搞错了，后面所有的推导都是空中楼阁。你的VAE为什么糊，SD的VAE为什么不糊。咱们先得给VAE平反。很多同学做实验，拿个MNIST或者CelebA数据集，跑个最基础的VAE，甚至连ResNet结构都不用，就几层卷积一堆，Loss函数就是一个MSE（均方误差）加上一个KL散度。跑完一看，重建出来的图，边缘全是虚的，头发丝这种高频细节根本看不见。这时候你就会得出一个结论：VAE不行，学不到高频信息，只会取平均值。这个结论在2015年是对的，但放在2024年甚至2021年（SD论文出来的时候）就是错的。传统的VAE之所以糊，核心原因在于MSE Loss。这玩意儿是像素级的对比，当模型对某个像素到底是黑是白拿不准的时候，为了让Loss最小，它就会取中间值——灰色。这就是模糊的根源。再加上KL散度强行要把Latent Space往标准正态分布上拉，这就导致模型很难去拟合数据分布里那些尖锐的、复杂的边缘分布（流形）。但是，Stable Diffusion用的那个VAE，全名叫AutoencoderKL，出自CompVis团队。这东西在训练的时候，加了猛料。它不仅仅用了MSE Loss，它还加了Perceptual Loss（感知损失）和PatchGAN Discriminator（判别器损失）。看到没有？重点来了。Perceptual Loss是用一个预训练好的VGG网络去提取特征，对比原图和重建图在深层特征上的差异，而不是仅仅对比像素。这意味着它关注的是纹理、结构这些“感知”层面的东西。Discriminator这东西就是GAN里的判别器。SD的VAE在训练后期，实际上引入了GAN的对抗训练机制。判别器会盯着重建图看，逼着Decoder去生成足够逼真的高频纹理，否则判别器就判你是假的。所以，SD里的那个VAE，本质上是一个拥有VAE结构的GAN。你去看看SD官方发布的VAE解码出来的图，哪怕没有Diffusion参与，仅仅是原图进Encoder再出Decoder，那个清晰度也是非常高的，虽然跟原图比会有极其细微的压缩痕迹，但绝对不是你印象中那种“糊成一团”。你去Github上搜一下CompVis/taming-transformers这个库，这是VQGAN的源头，也是后来Latent Diffusion用的VAE的前身。你要是想看代码细节，直接去HuggingFace的Diffusers库里看AutoencoderKL的源码。你会发现它的Loss配置里赫然写着LPIPS（这就是感知损失）和Discriminator。所以，结论一：SD用的VAE本身就不糊，它的Decoder甚至具备了超分（Super Resolution）的能力，因为它在训练阶段就被GAN Loss给调教过了。好，既然VAE不糊，那为什么还要Diffusion？为什么不直接用VAE生成？这就涉及到了生成模型的另一个痛点：Latent Space的分布太难搞了。VAE虽然能重建得不错，但它那个Latent Space（隐空间）如果不加干预，是非常复杂的。如果你从标准正态分布里随机采样一个点，扔进Decoder，出来的图很可能就是个鬼影，因为那个点可能根本就不在图像数据的流形上。为了让Latent Space规整，传统VAE加了KL散度，强制让它服从正态分布。但这又带来了过正则化的问题，导致生成能力下降。Stable Diffusion（或者说Latent Diffusion Models，LDM）的高明之处在于，它把问题拆解了：感知压缩阶段（Perceptual Compression）： 用一个强大的VAE（带GAN Loss的）把图像从像素空间（Pixel Space）压缩到隐空间（Latent Space）。这个阶段的目标只有一个：保真。我不求Latent分布多完美，我就求我压进去再解出来，肉眼看不出区别。SD 1.5把512x512的图压成了64x64x4，压缩比是8。语义生成阶段（Latent Diffusion）： 在这个64x64x4的隐空间里，训练一个Diffusion Model。这个Diffusion Model的任务不是生成像素，而是学习这个Latent Space的概率分布。为什么要冻结VAE？这就好比你要在一个操场上练跑步（训练Diffusion），你需要一个固定的操场（Latent Space）。如果你的VAE在训练Diffusion的时候还在动，还在调整参数，那就相当于你一边跑步，脚底下的地皮还在不断地地震、变形。这坐标系都乱了，Diffusion模型根本没法收敛。所以，我们先训练好VAE，让它形成一个稳定的、高质量的“低维投影空间”，把它锁死。然后让Diffusion在这个固定的空间里尽情地去学习数据分布。这种分阶段训练策略，是工程上的巨大胜利。它把“怎么画得像”和“画什么内容”这两个任务彻底解耦了。VAE负责“画得像”（纹理重建），Diffusion负责“画什么”（语义组合）。你觉得奇怪，为什么在隐层去噪之后，Decoder出来的图像就清晰了？这里有个非常反直觉的认知：清晰度（高频信息）其实并不完全存储在Latent里，而是存储在Decoder的权重里。Latent Code（那个64x64x4的张量）本质上是一个压缩的指令集。它告诉Decoder：“左上角这里大概是个眼睛，颜色是蓝的，形状是圆的”。但它不会告诉Decoder：“这个眼睛的虹膜纹理是怎样的，睫毛有几根”。那Decoder怎么知道睫毛怎么画？因为它在之前的预训练里（那个带GAN Loss的阶段）已经把“画睫毛”这个技能刻在自己的神经网络权重里了！当它收到“画眼睛”的指令时，它会调用自己权重里存储的纹理先验，把细节脑补出来。Diffusion的作用，就是生成一个极其精准的指令集（Latent）。在去噪的过程中，Diffusion模型不断地调整Latent的值，确保这些值所代表的结构关系、语义布局是完美的。一旦这个Latent生成好了，扔给Decoder，Decoder利用自己强大的重建能力，加上它自带的纹理补全功能（由GAN Loss训练得来），就能“渲染”出一张高清大图。所以，这整个过程不是“Diffusion去噪把图变清晰了”，而是：Diffusion生成了结构正确的骨架（Latent）。Frozen VAE Decoder利用自己的纹理库（权重）给这个骨架贴上了高清的皮。这就像是一个建筑师（Diffusion）画了一张非常精准的蓝图（Latent），虽然蓝图上没有画每一块砖的纹理，但他把墙的位置定得很准。然后施工队（Decoder）拿着蓝图，用最高级的材料（High-frequency Details）把楼盖了起来。场景案例： 你想想现在的AI修复老照片。输入的照片是模糊的，为什么输出能变清晰？是因为模型凭空“猜”出了细节。SD的Decoder也是一样的道理，它其实是一个生成式解码器。它不是在做无损解压，它是在做有损压缩后的联想恢复。你在问题里说：“VAE本身生成图像模糊，说明encoder、decoder以及中间的隐层表示没有学到本质的东西。”这个观点我要反驳一下。糊，不代表没学到本质。相反，糊，往往是因为只学到了本质，而丢弃了噪音。在图像处理中，什么是本质？语义、结构、颜色分布、物体关系，这些是本质。什么是噪音？地毯上每一根随机朝向的绒毛、墙壁上无规律的细微凹凸、相机感光元件带来的高频噪点。传统的MSE Loss训练出来的VAE，倾向于保留低频的“本质”，而抹平高频的“随机纹理”。因为它发现，把纹理抹平（取平均），Loss最小。所以，隐层表示恰恰是学到了最核心的语义结构。而SD之所以牛，是因为它引入了LDM（Latent Diffusion Model）。LDM意识到：既然Latent里只存了本质（低频），那我就专门在Latent里搞生成。至于那些丢掉的高频纹理，我交给Decoder去“脑补”回来。这就是一种极致的效率优化。你去看看CVPR上的那些Paper，比如High-Resolution Image Synthesis with Latent Diffusion Models（这就是SD的原作），里面明确提到了“Perceptual Compression”和“Semantic Compression”的区别。Perceptual Compression（感知压缩）： 去掉人类肉眼看不见的高频冗余，对应VAE。Semantic Compression（语义压缩）： 去掉纹理，只留结构和语义，对应Latent Space的设计。SD的成功，就在于它没有试图让一个模型同时做完这两件事。它让VAE做感知压缩，让Diffusion做语义生成。我在一线搞模型优化的时候，经常面临一个权衡：显存不够用了怎么办？训练太慢了怎么办？如果直接在像素空间跑Diffusion（比如OpenAI最早的那个DALL-E 2或者Google的Imagen的部分阶段），你要处理的是512x512x3的数据量，这计算量是巨大的。每一次去噪迭代都要算全图的卷积。把VAE冻结，把图像压到64x64x4。数据量直接少了多少？ (512*512*3) / (64*64*4) = 48倍！这不仅仅是快了一点点，这是让Consumer Grade GPU（消费级显卡）能跑得动SD的关键。如果没有这个Latent Space的压缩，SD根本不可能在你的RTX 3060上跑起来，它只能躺在Google的TPU机房里。所以，冻结VAE，在Latent上跑Diffusion，既是算法原理上的分治策略（纹理与语义解耦），也是工程落地上的必然选择（算力成本控制）。而且，这个冻结的VAE还是通用的。同一个VAE，你可以用来训练画猫的模型，也可以用来训练画建筑的模型。因为不管是猫还是建筑，它们底层的纹理构成（边缘、色彩斑块、梯度）在像素层面是共享的。这就像Photoshop的底层渲染引擎是通用的，不管你画什么图，像素的显示逻辑是一样的。优质资源插播： 建议你去读一下这篇博客：The Illustrated Stable Diffusion，作者是Jay Alammar。他把这个Pixel Space到Latent Space的转换讲得非常透彻，全是图解，非常适合建立直观理解。看完那个，你就明白为什么一定要在Latent里搞事情了。深入一点：流形假设与高维空间咱们再往深了聊一点，稍微硬核一点。数据科学里有个核心概念叫流形假设（Manifold Hypothesis）。它认为高维数据（比如图片）其实是分布在一个低维的流形上的。像素空间（512x512x3）维度极高，大部分区域都是无意义的噪声。你随机在这个空间取点，取出来的全是雪花点。 VAE训练的过程，就是试图找到这个低维流形，并把它映射到一个Latent Space里。但是，因为传统VAE能力有限，它找的这个流形比较“平滑”，也就是糊。它不敢在这个流形上搞出太陡峭的褶皱（高频细节），因为它怕过拟合。SD里的Frozen VAE，利用了GAN的能力，它映射出来的流形是非常精细的。但这个流形极其复杂，很难直接采样。Diffusion Model在做什么？它其实是在学习如何在Latent Space这个低维空间里游走。去噪的过程，就是把一个偏离流形的点（噪声），一步步拉回到流形上（清晰图像的Latent表示）的过程。因为VAE被冻结了，所以这个“流形”的形状是固定的。Diffusion只需要专心学怎么“上岸”，而不需要担心岸本身在移动。这就大大降低了训练难度。如果你让VAE和Diffusion一起训练（End-to-End），那就相当于你要在一个不断变形的充气城堡上练习走钢丝，这也就是为什么很多端到端的大模型极难训练、经常Collapse（坍塌）的原因。总结一下，为什么这套组合拳效果这么好？VAE并不弱： SD用的不是原始VAE，是带GAN Loss和Perceptual Loss的加强版VAE，它的重建本身就很清晰，具备纹理合成能力。分工明确： 冻结的VAE负责纹理这一层的物理规律（怎么让像素看起来像真的），Diffusion负责语义这一层的内容规律（怎么让形状组合起来像个猫）。降维打击： 在Latent Space做去噪，避开了像素空间的巨大计算量，也避开了像素级噪声的干扰，让模型能更专注地学习语义结构。先验知识： Decoder权重里存储了大量关于“自然图像该长什么样”的先验知识，它把Diffusion生成的简略指令“超分”成了丰富细节。所以，不要觉得VAE是拖后腿的。在SD这套架构里，VAE是那个默默无闻但功力深厚的扫地僧，它把最脏最累的像素搬运工作做完了，才让Diffusion这个天才少年能在Latent Space里挥洒才华。最后给个建议，如果你想在这一行深挖，千万别被老旧的教科书定义给框住了。现在的SOTA（State of the Art）模型，往往都是缝合怪。VAE、GAN、Diffusion、Transformer，大家早就不是非此即彼的关系了，而是像乐高积木一样被拆解、组合。你要关注的不是“哪个模型更好”，而是“每个模块擅长处理什么层级的信息”。这就是架构设计的艺术。你要是真感兴趣，可以试试自己训练一个小型的LDM，把VAE换成普通的，再换成VQ-VAE，对比一下效果，那感觉，比看一百篇知乎回答都来得实在。搞数据科学的，手如果不脏（Get your hands dirty），脑子永远是虚的。