# VAE
## 入门问题
第一次看论文的时候其实是懵的：
1.  $p_\theta(\mathcal z),p_\theta(\mathcal x|\mathcal z),p_\theta(\mathcal z|\mathcal x)$ 怎么理解，他们是同一个分布还是不同的分布？如果是不同分布为什么都用同一个下标 $\theta$？
2. 前边这一坨还没看懂，后边怎么突然就又搞了个 $q_\phi(\mathcal z|\mathcal x)$ ？p和q的区别是什么，为啥(z|x)还能有两个不同的分布？
> 为什么要搞这么多分布呢？苏剑林博客：我们的终极目标是为了求 $p(\mathbf x)$ ，但是这个没法直接求，因此只能走迂回战术 $p(\mathbf x)=\int p(\mathbf x|\mathbf z)p(\mathbf z)d\mathbf z$ ，我们假定一个隐藏变量z属于某个固定分布（比如标准高斯）那我们就能够通过在这个固定分布中采样一个z，然后根据z来生成x

关于第一个问题，许多博客都没有细说，论文里反而有一部分说明：
> We assume that the data are generated by some random process, involving an unobserved continuous random variable z. The process consists of two steps: (1) a value $\mathcal z^{(i)}$ is generated from some prior distribution $p_{\theta^*}(\mathcal z)$ ;(2)a value $\mathcal x^{(i)}$ is generated from some conditional distribution $p_{\theta^*}(\mathcal x|\mathcal z)$ .We assume that the prior $p_{\theta^*}(\mathcal z)$ and likelihood $p_{\theta^*}(\mathcal x|\mathcal z)$ come from parametric families of distributions $p_{\theta}(\mathcal z)$ and $p_{\theta}(\mathcal x|\mathcal z)$

论文里说： $p_\theta(\mathcal z),p_\theta(\mathcal x|\mathcal z)$ 分别是两个参数化的分布族（参数为 $\theta$ ，星号表示最优，即真实分布），他们分别表示隐状态向量分布和给定一个隐向量条件下数据的分布。
那为什么他们都是同一个下标 $\theta$ 呢？我们可以这样理解：这里的 $\theta,\phi$ 实际指的是模型参数（神经网络），  $p_{\theta}(\mathcal x|\mathcal z)$ 指的是将隐变量转换成真实数据的模型（生成模型）， $p_{\theta}(\mathcal z)$ 是这个模型内部的隐变量的分布，他们都是同一个模型的两个内部过程，可以把他们想像成一个整体。理论上这个模型当然也存在 $p_{\theta}(\mathcal z|\mathcal x)$，但是由于模型它是个生成模型所以无法直接求，得通过贝叶斯间接求，但是要走这条路就得有 $p_\theta(\mathcal x)$ ，就遇到了 $p_\theta(\mathcal x)$ 无法求解的问题，所以用生成模型直接求后验分布是不可行的(intractable)。
> 先验、后验、似然、证据是来源于贝叶斯定理的概念：
> $$P(H|D) = \frac {P(D|H)\cdot P(H)} {P(D)}$$
> 这个公式中，H表示隐藏状态，我们无法直接观测的，因此我们只能根据经验猜一个分布，这个分布叫先验(prior)；D表示显性状态，我们可以通过实验观察到，因此它的概率称为证据(evidence)；我们相信某个隐藏状态H的假设，求某个显性状态的概率P(D|H)是似然度(likelihood)；以及我们在观察到某个显性状态已经发生的情况下，对隐藏状态的分布为后验(posterior)P(H|D)
> - P(H)：先验，根据经验猜的
> - P(H|D)：后验，某个显性状态发生后的条件概率，一般作为目标
> - P(D|H)：似然度，相信隐藏状态假设，算某个显性状态发生的概率
> - P(D)：证据，即数据的边际概率(minal probability)
>
> 给定一个观测数据，求隐变量的后验分布 $p(\mathcal z|\mathcal x)$ 被称为**推断**任务，常用方法有VB(Variational Bayes)和MCMC(Markov Chain Monte Carlo)。VB也叫VBI(Variational Bayes Inference)。VAE利用了VB的理论基础，这就是它名字中"V"的由来。

关于第二个问题，论文中也有解释但是表述地不是很清晰，翁丽莲的解释：
>The optimal parameter $\theta^*$ is the one that maximizes the probability of generating real data samples:
$\theta^{*} = \displaystyle \arg\max_\theta \sum_{i=1}^n \log p_\theta(\mathbf{x}^{(i)})$，其中 $p_\theta(\mathbf{x}^{(i)}) = \int p_\theta(\mathbf{x}^{(i)}\vert\mathbf{z}) p_\theta(\mathbf{z}) d\mathbf{z}$
Unfortunately it is not easy to compute $p_\theta(\mathbf{x}^{(i)})$ in this way, as it is very expensive to check all the possible values of $\mathbf z$ and sum them up. To narrow down the value space to facilitate faster search, we would like to introduce a new approximation function to output what is a likely code given an input $\mathbf x$, $q_\phi(\mathbf{z}\vert\mathbf{x})$ parameterized by $\phi$.
![翁立莲的博客图片](/resource/vae_process_by_wenglilian.png)

说一下个人理解：我们无法在同一个模型（神经网络）上同时计算后验 $p_{\theta}(\mathcal z|\mathcal x)$ 和似然 $p_{\theta}(\mathcal x|\mathcal z)$ ，理由在上边第一个问题的回答里已经说过了。于是另开了一个模型 $q_\phi(\mathcal z|\mathcal x)$ ，它的参数为 $\phi$，用它来近似 $p_{\theta}(\mathcal z|\mathcal x)$ 。作者在这里套用了AE的概念，把 $q_\phi(\mathcal z|\mathcal x)$ 称为encoder，把 $p_{\theta}(\mathcal x|\mathcal z)$ 称为decoder。

现在我们知道了VAE的encoder和decoder是怎么来的，那么这两个东西具体是怎么工作的呢？
## 模型拆解
首先需要明确一点，VAE的论文正文的方法阐述部分并没有直接介绍encoder和decoder的具体模型，而是用p和q来表示这两个模型的分布，然后基于这两个分布提出了SGVB估计器和AEVB算法。然后在Example部分，作者用了一个神经网络模型来验证了这两个算法的有效性。


> 引自苏剑林博客：
> ![苏剑林的vae结构](/resource/vae_struct_by_sujianlin.png)


## 证据下界(evidence lower bound, ELBO)
$\log p_\theta(x)$ 的证据下界(ELBO)
```math
\mathcal{L}=-D_{KL}(q_{\phi}(\mathcal z|\mathcal x^{(i)})||p_{\theta}(\mathcal z))+\Bbb E_{q_{\phi}(\mathcal z|\mathcal x^{(i)})}[\log p_{\theta}(\mathcal x^{(i)}|\mathcal z)]
```
这个式子是用KL散度的定义推出来的
```math
D_{KL}(q(\mathcal z|\mathcal x)||p(\mathcal z|\mathcal x))=\Bbb E_{z\sim q}[\log q(\mathcal z|\mathcal x)-\log p(\mathcal z|\mathcal x)]
\newline=\Bbb E_{z\sim q}\Big[\log q(\mathcal z|\mathcal x) - \log \frac {p(\mathcal x|\mathcal z)p(\mathcal z)} {p(\mathcal x)}\Big]
\newline=\Bbb E_{z\sim q}\Big[\log q(\mathcal z|\mathcal x) - \log p(\mathcal x|\mathcal z) -\log p(\mathcal z) + \log p(\mathcal x)\Big ]
\newline=\log p(\mathcal x) +\Bbb E_{z\sim q}\Big[\log q(\mathcal z|\mathcal x) - \log p(\mathcal x|\mathcal z) -\log p(\mathcal z) \Big ]
```
观察到 $\log p(\mathcal x)$ 就是我们希望最大化的目标函数，因此改写此式为：
```math
\log p(\mathcal x)=D_{KL}(q(\mathcal z|\mathcal x)||p(\mathcal z|\mathcal x))+\Bbb E_{z\sim q}\Big[-\log q(\mathcal z|\mathcal x) + \log p(\mathcal x|\mathcal z) +\log p(\mathcal z) \Big ]\\
=D_{KL}(q(\mathcal z|\mathcal x)||p(\mathcal z|\mathcal x))- D_{KL}(q(\mathcal z|\mathcal x)||p(\mathcal z)) + \Bbb E_{z\sim q}[\log p(\mathcal x|\mathcal z)]
```
因为等是右边的第一项是KL散度，一定大于0,因此有
```math
\log p(\mathcal x)\ge - D_{KL}(q(\mathcal z|\mathcal x)||p(\mathcal z)) + \Bbb E_{z\sim q}[\log p(\mathcal x|\mathcal z)]
```
不等号右边即为证据下界
> 证据下界中的“证据”(evidence)指的是数据的边际概率，即 $p_\theta(\mathcal x)$。VAE的目标是最大化 $\log p_\theta(x)$ ，根据上边的推导，它的下界就是证据下界。

## 损失函数
上边得到的ELBO是根据散度定义和贝叶斯定理推导而来的，是一个VB的核心结论。我们的目的是最大化 $\log p_\theta(\mathcal x)$，等价于求它上限的最大化，即求
```math
\theta^*=\displaystyle \arg\max_\theta ELBO
```
我们习惯于将损失函数定义为一个求最小的函数，因此
```math
\theta^*=\displaystyle \arg\min_\theta -ELBO
```
所以损失函数为
```math
L=D_{KL}(q_\phi(\mathcal z|\mathcal x)||p_\theta(\mathcal z)) + \Bbb E_{z\sim q_\phi}[-\log p_\theta(\mathcal x|\mathcal z)]
```
第一项称为正则项，第二项称为重构误差。
> 为什么KL散度是正则项？每个输入的数据都会产生一个独立的正态分布（均值和方差），重参数化后即加入了基于这个方差的噪声进行训练。训练过程肯定会逐渐让噪声归零，也就是会让方差趋近于0，这其实就回到了AE。为了避免这这种情况，我们需要加一个正则，让方差趋近于1，即让 $q_\phi$ 去逼近标准正态分布[3]。

这里重点说一下重构误差项。原论文用SGVB估计器来估计这个重构误差项，还专门设计了一套AEVB的算法，其实就是重参数化+蒙特卡洛估计。但是很多代码实现中，重构误差项直接用MSE来估计，很多人这里都没讲清楚。余正阳的视频[4]中有提到，这个估计实际等于一个MSE，但并未给出原理推导。
这里其实是把“期望”用蒙特卡洛采样近似掉了，同时由于论文假设了似然分布是一个高斯分布，
此时重构项的期望通常用 L 次采样的平均 近似：
```math
\mathbb{E}_{q(z|x)}[\log p(x|z)] \approx \frac{1}{L}\displaystyle \sum _{l=1}^L \log p(x|z_l),\quad z_l\sim q(z|x)
```

```math
\log p_\theta(\mathcal x|\mathcal z)=\log \mathcal N(\mathcal x; \mathcal \mu_\theta(\mathcal z), \mathcal \sigma^2_\theta I)
```

有视频说，VAE的损失函数其实就是这两个项之间的博弈：一方面，重构项缩小就要求噪声尽量低，就会让encoder输出的方差减少到0；而另一方面正则项要缩小就要求encoder输出的方差要趋近于1。这两个项相互博弈才让VAE找到一种平衡，能输出图片。
## 重参数化
先看论文原文对重参数化的描述：
> Let z be a continuous random variable, and $\mathcal z \sim q_\phi(\mathcal z|\mathcal x)$ be some conditional distribution. It is then often possible to express the random variable z as a deterministic variable $\mathcal z = g_\phi(\epsilon, \mathcal x)$ , where $\epsilon$ is an auxiliary variable with independent marginal $p(\epsilon)$ , and $g_\phi(.)$ is some vector-valued function parameterized by φ.

理解一下，我们取z的过程是从某个分布中随机采样，这个过程呢在训练中无法求梯度，因为随机采样过程不可导。而重参数化就是为了解决这个问题，我们把这个随机采样的过程等价于一个确定性的函数 $g_\phi(\epsilon, \mathcal x)$ ，其中 $\epsilon$ 可以看成一个随机噪声，服从某个确定性分布 $p(\epsilon)$ 。

## VAE和VB的关系
我们的任务是计算后验概率 $P(z|\mathcal x)$ ，这叫做推断问题。这个问题有两个思路：
1. 用一个确定的分布 $q(\mathcal z|\mathcal x)$ 去近似 $p(z|\mathcal x)$ ，这就是变分推断。
2. 用MCMC等方法去采样 $p(z|\mathcal x)$ ，这就是MCMC推断。

变分推断也叫变分贝叶斯推断(VB,VBI)，它的本质是把推断问题转化成优化问题：在一个函数族 $\mathcal F$ 中寻找一个相对简单的分布 $q(\mathcal z|\mathcal x)$ 去近似 $p(z|\mathcal x)$ ，想让他们之间的KL散度 $D_{KL}(q(\mathcal z|\mathcal x)||P(z|\mathcal x))$ 最小，由此引出ELBO，将问题转化成求一个函数 $q(\mathcal z|\mathcal x)$ 使ELBO最大。

变分推断最常用的方法叫Mean-Field，也叫MFVB。此外还有Black Box Variational Inference等方法。这两个在VAE论文中都有所提及（主要是MFVB）。
>  The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. Unfortunately, the common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intractable in the general case.

> Note that in contrast with the approximate posterior in mean-field variational inference, it is not necessarily factorial and its parameters φ are not computed from some closed-form expectation. Instead, we’ll introduce a method for learning the recognition model parameters φ jointly with the generative model parameters θ.

> Classical mean-field VB assumes a factorized approximate posterior followed by a closed form optimization updates (which usually required conjugate priors).——来自另一篇综述论文[2]

> 补充知识：
> - Classical mean-field VB assumes a factorized approximate posterior 这句话中的factorized指的是 $q(\mathcal z)$ 是分解形式，即 $q(\mathcal z) = \prod_{i=1}^d q(z_i)$ ，其中 $z_i$ 是 $z$ 的第 $i$ 个维度（类似朴素贝叶斯的条件独立假设），这也是MFVB的核心假设。
> - 闭式解（Closed-Form Solution），即能用数学公式直接写出最优解（如通过求导得到解析表达式），而非依赖数值迭代（如梯度下降）。

由此可以看出，ELBO概念以及用q去近似p(z|x)这个想法，都是VB所提出的，VAE只是借用。VAE真正的创新在在于：
1. 摒弃了MF方法，使用一个不需要解析解和因子分解假设的优化方法，用一个全局编码器网络作为q，相较于MFVB的每个隐变量维度单独进行优化，VAE的优化目标是全局的。

2. 重参数化技巧
## 参考资料
> [1] [VAE论文](https://arxiv.org/pdf/1312.6114)  
> [2] [关于AE、VAE的一篇综述论文](https://arxiv.org/pdf/2003.05991)  
> [3] [苏剑林博客](https://kexue.fm/archives/5253)  
> [4] [翁丽莲博客](https://lilianweng.github.io/posts/2018-08-12-vae/)   
> [5] [什么是推断、变分推断、变分贝叶斯推断](https://zhuanlan.zhihu.com/p/575328650)  
> [6] [余正阳视频](https://www.bilibili.com/video/BV1Mgh4zJEZV)   
